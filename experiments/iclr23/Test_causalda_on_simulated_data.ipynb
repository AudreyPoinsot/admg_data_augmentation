{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test CausalDA on simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# add package path to sys\n",
    "import sys\n",
    "sys.path.insert(0, 'c:\\\\git\\\\Causal Data Augmentation\\\\ADMG') # TO CHANGE to match your current folder\n",
    "\n",
    "# my SCM simulator\n",
    "from experiments.suite.data.simulations import load_data as causal_data_simu\n",
    "\n",
    "# from CausalDA\n",
    "from causal_data_augmentation.api_support.experiments.logging.pickler import Pickler\n",
    "from causal_data_augmentation.experiment_api import CausalDataAugmentationEagerTrainingExperimentAPI\n",
    "from causal_data_augmentation.contrib.eager_augmentation_evaluators import PredictionEvaluator, PropertyEvaluator\n",
    "from causal_data_augmentation.contrib.aug_predictors.xgb import AugXGBRegressor\n",
    "from causal_data_augmentation.api_support.experiments.logging.pickler import Pickler\n",
    "import causal_data_augmentation.causal_data_augmentation.api_support.method_config as method_config_module\n",
    "from ananke.graphs import ADMG\n",
    "from causal_data_augmentation.causal_data_augmentation.api import EagerCausalDataAugmentation\n",
    "from causal_data_augmentation.causal_data_augmentation.api import AugmenterConfig, FullAugmentKind\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Tuple, Iterable, Optional, Union\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# evaluation protocol\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_percentage_error\n",
    "import torch\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy\n",
    "from statsmodels.stats.weightstats import DescrStatsW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Code to call CausalDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-1- Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global Parameters\n",
    "\n",
    "param_grid_search = {\n",
    "    'n_estimators': [10, 50, 200],\n",
    "    'reg_lambda': [1, 10, 100]\n",
    "}\n",
    "\n",
    "save_models = False\n",
    "\n",
    "\n",
    "### CausalDA Parameters\n",
    "\n",
    "fit_to_aug_only = True\n",
    "\n",
    "augmenter_config_name = 'FullAugment'\n",
    "augmenter_config = {\n",
    "    'normalize_threshold_by_data_size': True,\n",
    "    'weight_threshold': 1e-2, \n",
    "    'weight_threshold_type': 'total',\n",
    "\n",
    "    'weight_kernel_cfg': {\n",
    "        'type': 'vanilla_kernel',\n",
    "\n",
    "        'conti_kertype': 'gaussian', # type of kernel to use for a continuous variable\n",
    "        'conti_bw_method': 'normal_reference',\n",
    "        'conti_bw_temperature': 1,\n",
    "\n",
    "        'ordered_kertype': 'indicator', # type of kernel to use for a discrete and ordered variable\n",
    "        'ordered_bw_method': 'indicator',\n",
    "\n",
    "        'unordered_kertype': 'indicator', # type of kernel to use for a discrete and unordered variable\n",
    "        'unordered_bw_method': 'indicator',\n",
    "\n",
    "        'const_bandwidth': False, # False: adapt the bandwidth to the type of kerel used\n",
    "        'bandwidth_temperature': 0.001 # bandwidth used in case of constant bandwidth\n",
    "    }\n",
    "}\n",
    "\n",
    "aug_coeff = [0.5]\n",
    "\n",
    "\n",
    "### Evaluation parameters\n",
    "\n",
    "evaluators_param = [(mean_squared_error,'XGB_MSE')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Utils to run and evaluate CausalDA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-2-a Proposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmenter(augmenter_config: AugmenterConfig, \n",
    "                    method: EagerCausalDataAugmentation, \n",
    "                    data: pd.DataFrame, \n",
    "                    admg: ADMG) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Perform the augmentation using the augmenter configured by ``augmenter_config``.\n",
    "\n",
    "    Parameters:\n",
    "        augmenter_config : Method configuration.\n",
    "        method : Instantiated method object.\n",
    "        data : Data to be augmented.\n",
    "        admg : ADMG to be used for the augmentation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing\n",
    "\n",
    "        - augmented_data : The augmented data DataFrame.\n",
    "        - aug_weights : The instance weights corresponding to the augmented data.\n",
    "    \"\"\"\n",
    "    if isinstance(augmenter_config, FullAugmentKind):\n",
    "        augmented_data, aug_weights = method.augment(data, admg)\n",
    "        aug_weights = aug_weights.flatten()\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return augmented_data, aug_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _augment(data: pd.DataFrame, \n",
    "             graph, \n",
    "             augmenter_config: AugmenterConfig, \n",
    "             data_cache_base_path, \n",
    "             data_cache_name) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "        \"\"\"Instantiate the method and perform the data augmentation.\n",
    "\n",
    "        Parameters:\n",
    "            data : Data to be augmented.\n",
    "            graph : ADMG to be used for the augmentation.\n",
    "            augmenter_config : Method configuration.\n",
    "            data_cache_base_path: The path to the folder to save the trained model and the augmented data\n",
    "            data_cache_name: The base name that the saved files should follow (it contains the experiment settings)\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing\n",
    "\n",
    "            - augmented_data : The augmented data DataFrame.\n",
    "            - aug_weights : The instance weights corresponding to the augmented data.\n",
    "        \"\"\"\n",
    "        vertices, di_edges, bi_edges = graph\n",
    "        admg = ADMG(vertices, di_edges=di_edges, bi_edges=bi_edges)\n",
    "        method = EagerCausalDataAugmentation(data_cache_base_path, data_cache_name, augmenter_config)\n",
    "\n",
    "        # Augment\n",
    "        augmented_data, aug_weights = apply_augmenter(\n",
    "            augmenter_config, method, data, admg)\n",
    "        return augmented_data, aug_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_method(data: pd.DataFrame, \n",
    "               graph,\n",
    "               predicted_var_name: str,\n",
    "               predictor_model,\n",
    "               augmenter_config: AugmenterConfig,\n",
    "               aug_coeff,\n",
    "               fit_to_aug_only,\n",
    "               data_cache_base_path, \n",
    "               data_cache_name):\n",
    "    \"\"\"Run the method and record the results.\n",
    "\n",
    "    Parameters:\n",
    "        data: The data to be augmented.\n",
    "        graph: The ADMG object used for performing the augmentation.\n",
    "        predicted_var_name: The name of the predicted variable.\n",
    "        predictor_model: Trainable predictor model to be trained on the augmented data. Should implement ``fit()`` and ``predict()``.\n",
    "        augmenter_config: AugmenterConfig,\n",
    "        aug_coeff: Regularization term in for the augmented data\n",
    "        fit_to_aug_only: Whether or not to fit the models only to the augmented data\n",
    "        data_cache_base_path: The path to the folder to save the trained model and the augmented data\n",
    "        data_cache_name: The base name the saved files should follow (it contains the experiment settings)\n",
    "    \n",
    "    Returns:\n",
    "            List of trained models\n",
    "    \"\"\"\n",
    "    # Augment data\n",
    "    augmented_data, aug_weights = _augment(data, graph, augmenter_config, data_cache_base_path, data_cache_name)\n",
    "    \n",
    "    # Save augmented data and weights \n",
    "    augmented_data_to_save_df = augmented_data.copy()\n",
    "    augmented_data_to_save_df['aug_weights'] = aug_weights\n",
    "    _augmented_data_pickler = Pickler(data_cache_name + \"_augmented\", data_cache_base_path)\n",
    "    _augmented_data_pickler.save(augmented_data_to_save_df)\n",
    "    \n",
    "    # self._measure_augmentation(augmented_data, aug_weights, data))\n",
    "\n",
    "    model_list = []\n",
    "    predictor = deepcopy(predictor_model)\n",
    "    for aug_coeff in aug_coeff:\n",
    "        # Perform training\n",
    "        if fit_to_aug_only:\n",
    "            augmented_data = None\n",
    "            orig_weights = np.zeros(len(data))\n",
    "        else:\n",
    "            X = np.array(data.drop(predicted_var_name, axis=1))\n",
    "            Y = np.array(data[[predicted_var_name]])\n",
    "            aug_X = np.array(augmented_data.drop(predicted_var_name, axis=1))\n",
    "            aug_Y = np.array(augmented_data[[predicted_var_name]])\n",
    "            orig_weights = np.ones(len(data)) / len(data)\n",
    "            if aug_weights.size > 0:\n",
    "                orig_weights *= 1 - aug_coeff\n",
    "                aug_weights *= aug_coeff\n",
    "\n",
    "        orig_weights *= len(data)\n",
    "        aug_weights *= len(data)\n",
    "\n",
    "        predictor.fit(data, augmented_data, orig_weights, aug_weights)\n",
    "        model_list.append(predictor.model)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_proposed(dataset, \n",
    "                       param_grid, \n",
    "                       evaluators_param, \n",
    "                       augmenter_config_name, \n",
    "                       augmenter_config, \n",
    "                       fit_to_aug_only, \n",
    "                       aug_coeff, \n",
    "                       data_cache_base_path, \n",
    "                       data_cache_name, \n",
    "                       debug=True,\n",
    "                       save_models=False):\n",
    "    \"\"\"Apply the proposed method, CausalDA, and evaluate it.\n",
    "\n",
    "    Args:\n",
    "        dataset (tuple): train_data (dataframe), test_data (dataframe), graph (list of edges), predicted_var_name (string)\n",
    "        param_grid (dict): {'n_estimators': list of n_estimators to optimize on, 'reg_lambda': list of n_estimators to optimize on}\n",
    "        evaluators_param (list): list of tuples (eval_metric (function), name (string))\n",
    "        augmenter_config_name: The name of the type of augmenter\n",
    "        augmenter_config: AugmenterConfig\n",
    "        fit_to_aug_only: Whether or not to fit the models only to the augmented data\n",
    "        aug_coeff: Regularization term in for the augmented data\n",
    "        data_cache_base_path: The path to the folder to save the trained model and the augmented data\n",
    "        data_cache_name: The base name the saved files should follow (it contains the experiment settings)\n",
    "        debug: Whether or not to run the causal_data_augmentation in debug mode printing comments\n",
    "        save_models: Whether or not to save the trained models\n",
    "\n",
    "    Returns:\n",
    "        evaluators_res: liste of the scores given by the evaluators on the models trained with augmentation\n",
    "    \"\"\"\n",
    "\n",
    "    ###################\n",
    "    ## Get dataset\n",
    "    ###################\n",
    "    train_data, test_data, graph, predicted_var_name = dataset\n",
    "    test_X = test_data.drop(predicted_var_name, axis=1)\n",
    "    test_Y = test_data[predicted_var_name]\n",
    "    test_data = (test_X, test_Y)\n",
    "\n",
    "    ###################\n",
    "    ## Prepare evaluation\n",
    "    ###################\n",
    "    predictor_model = AugXGBRegressor(predicted_var_name, param_grid)\n",
    "    AugmenterConfigClass = getattr(method_config_module, augmenter_config_name)\n",
    "    augmenter_config = AugmenterConfigClass(**augmenter_config)\n",
    "\n",
    "    ###################\n",
    "    ## Run\n",
    "    ###################\n",
    "    model_list = run_method(train_data, \n",
    "                            graph,\n",
    "                            predicted_var_name,\n",
    "                            predictor_model, \n",
    "                            augmenter_config, \n",
    "                            aug_coeff,\n",
    "                            fit_to_aug_only,\n",
    "                            data_cache_base_path, \n",
    "                            data_cache_name)\n",
    "    # Save XGB models\n",
    "    if save_models:\n",
    "        for ind_mod, mod in enumerate(model_list):\n",
    "            mod.save_model(str(data_cache_base_path) + '/' + str(data_cache_name) + '_xgb_proposed_' + str(ind_mod) + '.json')\n",
    "    \n",
    "    ###################\n",
    "    ## Evaluate\n",
    "    ###################\n",
    "    evaluators_res = []\n",
    "    for mod in model_list:\n",
    "        mod_res = []\n",
    "        for ev in evaluators_param:\n",
    "            pred = mod.predict(test_X)\n",
    "            mod_res.append((ev[1],ev[0](test_Y,pred)))\n",
    "        evaluators_res.append(mod_res)\n",
    "    \n",
    "    return evaluators_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I-2-b Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_baseline_xgb(dataset, \n",
    "                           param_grid,\n",
    "                           evaluators_param,\n",
    "                           data_cache_base_path, \n",
    "                           data_cache_name,\n",
    "                           debug=True,\n",
    "                           save_models=False):\n",
    "    \"\"\"Apply the baseline method, simple XGB, and evaluate it.\n",
    "\n",
    "    Args:\n",
    "        dataset (tuple): train_data (dataframe), test_data (dataframe), graph (list of edges), predicted_var_name (string)\n",
    "        param_grid (dict): {'n_estimators': list of n_estimators to optimize on, 'reg_lambda': list of n_estimators to optimize on}\n",
    "        evaluators_param (list): list of tuples (eval_metric (function), name (string))\n",
    "        data_cache_base_path: The path to the folder to save the trained model and the augmented data\n",
    "        data_cache_name: The base name the saved files should follow (it contains the experiment settings)\n",
    "        debug: Whether or not to run the causal_data_augmentation in debug mode printing comments\n",
    "        save_models: Whether or not to save the trained models\n",
    "\n",
    "    Returns:\n",
    "        evaluators_res: liste of the scores given by the evaluators on the models trained with augmentation\n",
    "    \"\"\"\n",
    "    train_data, test_data, graph, predicted_var_name = dataset\n",
    "    test_X = test_data.drop(predicted_var_name, axis=1)\n",
    "    test_Y = test_data[predicted_var_name]\n",
    "    test_data = (test_X, test_Y)\n",
    "\n",
    "    # Prepare evaluation\n",
    "    predictor_model = AugXGBRegressor(predicted_var_name, param_grid)\n",
    "\n",
    "    sample_weight = np.ones((len(train_data)))\n",
    "    predictor_model.fit(train_data, None, sample_weight=sample_weight)\n",
    "            \n",
    "    # Save XGB model\n",
    "    if save_models:\n",
    "        print(str(data_cache_base_path) + '/' + str(data_cache_name) + '_xgb_baseline.json')\n",
    "        predictor_model.model.save_model(str(data_cache_base_path) + '/' + str(data_cache_name) + '_xgb_baseline.json')\n",
    "        \n",
    "    # Evaluate\n",
    "    evaluators_res = []\n",
    "    for ev in evaluators_param:\n",
    "        pred = predictor_model.predict(test_X)\n",
    "        evaluators_res.append((ev[1],ev[0](test_Y,pred)))\n",
    "    \n",
    "    return evaluators_res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3 Original paper example as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper example : X1 <-- Y --> X2\n",
    "\n",
    "run_paper_example = False\n",
    "\n",
    "if run_paper_example:\n",
    "    vertices = ['V1', 'V2', 'V3']\n",
    "    di_edges = [('V1', 'V2'), ('V1', 'V3')]\n",
    "    bi_edges = []\n",
    "\n",
    "    data_list = []\n",
    "    for i in range(2):\n",
    "        for j in range(3*i,3*i+3):\n",
    "            data_list.append([i,j,j])\n",
    "    #print(np.array(data_list))\n",
    "    data = pd.DataFrame(np.array(data_list), columns=vertices)\n",
    "\n",
    "    predicted_var_name = 'V3'\n",
    "\n",
    "    data_cache_base_path = ''\n",
    "    data_cache_base_path = Path(data_cache_base_path)\n",
    "    data_cache_name = 'simu_test'\n",
    "\n",
    "    # Intermediate arguments\n",
    "    AugmenterConfigClass = getattr(method_config_module, augmenter_config_name)\n",
    "    augmenter_config_ok = AugmenterConfigClass(**augmenter_config)\n",
    "    method = EagerCausalDataAugmentation(data_cache_base_path, data_cache_name, augmenter_config_ok)\n",
    "\n",
    "    # admg = ADMG(vertices, di_edges=di_edges, bi_edges=bi_edges)\n",
    "    # aug_data, aug_weights = apply_augmenter(augmenter_config=augmenter_config_ok, \n",
    "    #                                         method=method, \n",
    "    #                                         data=data, \n",
    "    #                                         admg=admg)\n",
    "\n",
    "    graph = (vertices, di_edges, bi_edges)\n",
    "    aug_data, aug_weights = _augment(data, \n",
    "                                    graph, \n",
    "                                    augmenter_config_ok, \n",
    "                                    data_cache_base_path, \n",
    "                                    data_cache_name)\n",
    "\n",
    "    aug_data_to_print = aug_data.copy()\n",
    "    aug_data_to_print['weight'] = aug_weights*data.shape[0]\n",
    "    print(aug_data_to_print)\n",
    "    print(aug_data_to_print['weight'].sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-0- Utils functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL-div, Wasserstein and MMD distances computation from samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMD(x, y, kernel):\n",
    "    \"\"\"Emprical maximum mean discrepancy. The lower the result, the more evidence that distributions are the same.\n",
    "    code from : https://www.onurtunali.com/ml/2019/03/08/maximum-mean-discrepancy-in-machine-learning.html\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as \"multiscale\" or \"rbf\"\n",
    "    \"\"\"\n",
    "    xx, yy, zz = torch.mm(x, x.t()), torch.mm(y, y.t()), torch.mm(x, y.t())\n",
    "    rx = (xx.diag().unsqueeze(0).expand_as(xx))\n",
    "    ry = (yy.diag().unsqueeze(0).expand_as(yy))\n",
    "\n",
    "    dxx = rx.t() + rx - 2. * xx # Used for A in (1)\n",
    "    dyy = ry.t() + ry - 2. * yy # Used for B in (1)\n",
    "    dxy = rx.t() + ry - 2. * zz # Used for C in (1)\n",
    "\n",
    "    XX, YY, XY = (torch.zeros(xx.shape),\n",
    "                  torch.zeros(xx.shape),\n",
    "                  torch.zeros(xx.shape))\n",
    "\n",
    "    if kernel == \"multiscale\":\n",
    "        bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n",
    "        for a in bandwidth_range:\n",
    "            XX += a**2 * (a**2 + dxx)**-1\n",
    "            YY += a**2 * (a**2 + dyy)**-1\n",
    "            XY += a**2 * (a**2 + dxy)**-1\n",
    "\n",
    "    if kernel == \"rbf\":\n",
    "        bandwidth_range = [10, 15, 20, 50]\n",
    "        for a in bandwidth_range:\n",
    "            XX += torch.exp(-0.5*dxx/a)\n",
    "            YY += torch.exp(-0.5*dyy/a)\n",
    "            XY += torch.exp(-0.5*dxy/a)\n",
    "            \n",
    "    return torch.mean(XX + YY - 2. * XY)\n",
    "\n",
    "\n",
    "def KL_div(x, y, x_weights=None, y_weights=None, kernel='gaussian'):\n",
    "    \"\"\"Emprical Kullback Leibler divergence estimation from Kernel Density Estimates of two distributions defined by their samples.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as 'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "    \"\"\"\n",
    "    # Estimate Kernel Density\n",
    "    if x_weights is not None:\n",
    "        kde_x = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(x, sample_weight=x_weights)\n",
    "    else:\n",
    "        kde_x = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(x)\n",
    "    if y_weights is not None:\n",
    "        kde_y = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(y, sample_weight=y_weights)\n",
    "    else:\n",
    "        kde_y = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(y)\n",
    "    \n",
    "    # Compute density of samples in x and y according to x's law and y's law \n",
    "    x_plus_y = np.concatenate((x, y), axis=0)\n",
    "    p_x = kde_x.score_samples(x_plus_y)\n",
    "    q_y = kde_y.score_samples(x_plus_y)\n",
    "    \n",
    "    # Compute kl div\n",
    "    kl_div = scipy.stats.entropy(p_x, q_y)\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "def wasserstein_distance(x, y, x_weights=None, y_weights=None, kernel='gaussian'):\n",
    "    \"\"\"Emprical Wasserstein distance estimation from Kernel Density Estimates of two distributions defined by their samples.\n",
    "\n",
    "    Args:\n",
    "        x: first sample, distribution P\n",
    "        y: second sample, distribution Q\n",
    "        kernel: kernel type such as 'gaussian', 'tophat', 'epanechnikov', 'exponential', 'linear', 'cosine'\n",
    "    \"\"\"\n",
    "    # Estimate Kernel Density from datasets\n",
    "    if x_weights is not None:\n",
    "        kde_x = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(x, sample_weight=x_weights)\n",
    "    else:\n",
    "        kde_x = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(x)\n",
    "    if y_weights is not None:\n",
    "        kde_y = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(y, sample_weight=y_weights)\n",
    "    else:\n",
    "        kde_y = KernelDensity(kernel=kernel, bandwidth=\"silverman\").fit(y)\n",
    "    \n",
    "    # Compute density of samples in x and y according to x's law and y's law \n",
    "    x_plus_y = np.concatenate((x, y), axis=0)\n",
    "    p_x = kde_x.score_samples(x_plus_y)\n",
    "    q_y = kde_y.score_samples(x_plus_y)\n",
    "    \n",
    "    # Compute kl div\n",
    "    w_dist = scipy.stats.wasserstein_distance(p_x, q_y)\n",
    "    return w_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test distances\n",
    "\n",
    "test_distance_functions = False\n",
    "\n",
    "if test_distance_functions:\n",
    "    data = causal_data_simu.generate_data(\n",
    "            causal_mechanism='nn', #['linear', 'polynomial', 'sigmoid_add', 'sigmoid_mix', 'gp_add', 'gp_mix', 'nn']\n",
    "            nodes=10,\n",
    "            expected_degree = 3,\n",
    "            noise='gaussian', # 'gaussian', 'uniform' or a custom noise function\n",
    "            noise_coeff=.4,\n",
    "            dag_type='erdos',\n",
    "            npoints=1000\n",
    "        )\n",
    "\n",
    "    data_x = data.iloc[:500]\n",
    "    data_y = data.iloc[500:]\n",
    "    weights_x = np.ones(500)\n",
    "    weights_y = 0.5*np.ones(500)\n",
    "\n",
    "    print(KL_div(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian'))\n",
    "    print(KL_div(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x, y_weights=weights_x))\n",
    "    print(KL_div(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x/500, y_weights=weights_x/500))\n",
    "    print(KL_div(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x, y_weights=weights_y))\n",
    "    print(wasserstein_distance(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian'))\n",
    "    print(wasserstein_distance(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x, y_weights=weights_x))\n",
    "    print(wasserstein_distance(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x/500, y_weights=weights_x/500))\n",
    "    print(wasserstein_distance(data_x.to_numpy(), data_y.to_numpy(), kernel='gaussian', x_weights=weights_x, y_weights=weights_y))\n",
    "    print(float(MMD(torch.tensor(data_x.values), torch.tensor(data_y.values), kernel='rbf').numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Augment data and fit XGB function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data_fit_and_evaluate_xgb(data, \n",
    "                                      train_size, \n",
    "                                      augmenter_config_name, \n",
    "                                      augmenter_config, \n",
    "                                      graph_,\n",
    "                                      data_cache_base_path,\n",
    "                                      data_cache_name,\n",
    "                                      save_data,\n",
    "                                      run_results_df,\n",
    "                                      aggregated_results_df\n",
    "                                      ):\n",
    "    ### Build the dataset\n",
    "    predicted_var_name = data.columns[-1] # Fake varaible to predict --> only to use train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[predicted_var_name]), data[predicted_var_name], train_size=train_size)\n",
    "    train_data = pd.concat((X_train, y_train), axis=1)\n",
    "    test_data = pd.concat((X_test, y_test), axis=1)\n",
    "    \n",
    "    ### Apply method\n",
    "    AugmenterConfigClass = getattr(method_config_module, augmenter_config_name)\n",
    "    augmenter_config_ = AugmenterConfigClass(**augmenter_config)\n",
    "    aug_data, aug_weights = _augment(train_data, \n",
    "                                    graph_, \n",
    "                                    augmenter_config_, \n",
    "                                    data_cache_base_path, \n",
    "                                    data_cache_name)\n",
    "    \n",
    "    ### Save data\n",
    "    if save_data :\n",
    "        # Augmented data\n",
    "        augmented_data_to_save_df = aug_data.copy()\n",
    "        augmented_data_to_save_df['aug_weights'] = aug_weights\n",
    "        _augmented_data_pickler = Pickler(data_cache_name + \"_augmented\", data_cache_base_path)\n",
    "        _augmented_data_pickler.save(augmented_data_to_save_df)\n",
    "        # Train data\n",
    "        _train_data_pickler = Pickler(data_cache_name + \"_data\", data_cache_base_path)\n",
    "        _train_data_pickler.save(train_data)\n",
    "        # Test data\n",
    "        _test_data_pickler = Pickler(data_cache_name + \"_test\", data_cache_base_path)\n",
    "        _test_data_pickler.save(test_data)\n",
    "    \n",
    "    ### Compute stats on augmented data VS train data : \n",
    "    aug_data_df = aug_data.copy()\n",
    "    aug_data_df['aug_weights'] = aug_weights\n",
    "    train_df_join_aug = train_data.merge(aug_data_df, how=\"left\", on=list(train_data.columns))\n",
    "    run_results_df['frac_filtered'] = [train_df_join_aug[train_df_join_aug['aug_weights'].isna()].shape[0] / train_data.shape[0]]\n",
    "    \n",
    "    train_data_df = train_data.copy()\n",
    "    train_data_df['col'] = 1\n",
    "    aug_df_join_train = aug_data_df.merge(train_data_df, how=\"left\", on=list(aug_data.columns))\n",
    "    run_results_df['frac_augmented'] = [aug_df_join_train[aug_df_join_train['col'].isna()].shape[0] / train_data.shape[0]]\n",
    "    \n",
    "    run_results_df['avg_weight_aug_in_train'] = [aug_df_join_train[aug_df_join_train['col']==1].aug_weights.mean()]\n",
    "    run_results_df['avg_weight_aug_NOT_in_train'] = [aug_df_join_train[aug_df_join_train['col'].isna()].aug_weights.mean()]\n",
    "    run_results_df['sum_weight_aug_in_train'] = [aug_df_join_train[aug_df_join_train['col']==1].aug_weights.sum()]\n",
    "    run_results_df['sum_weight_aug_NOT_in_train'] = [aug_df_join_train[aug_df_join_train['col'].isna()].aug_weights.sum()]\n",
    "    run_results_df['SUM_aug_weights'] = [aug_data_df['aug_weights'].sum()]\n",
    "    \n",
    "    run_results_df['variance_train_NOT_weighted'] = [np.var(train_data.to_numpy(), axis=0)]\n",
    "    run_results_df['variance_aug_NOT_weighted'] = [np.var(aug_data.to_numpy(), axis=0)]\n",
    "    run_results_df['variance_aug_weighted'] = [DescrStatsW(aug_data.to_numpy(), weights=aug_weights, ddof=0).var]\n",
    "    \n",
    "    if aug_data.shape[0] > 2 : # minimum 10 samples to compute an estimate\n",
    "        run_results_df['KL_div_train_vs_aug_NOT_weighted'] = [KL_div(aug_data.to_numpy(), train_data.to_numpy(), kernel='gaussian')]\n",
    "        run_results_df['KL_div_train_vs_aug_weighted'] = [KL_div(aug_data.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights)]\n",
    "        run_results_df['Wasserstein_dist_train_vs_aug_NOT_weighted'] = [wasserstein_distance(aug_data.to_numpy(), train_data.to_numpy(), kernel='gaussian')]\n",
    "        run_results_df['Wasserstein_dist_train_vs_aug_weighted'] = [wasserstein_distance(aug_data.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights)]\n",
    "\n",
    "    \n",
    "    ### Evaluate XGB models & Fill results table\n",
    "    for pred_var in train_data.columns:\n",
    "        \n",
    "        # predicted variable caracteristics\n",
    "        run_results_df['predicted_variable'] = [pred_var]\n",
    "        if len(list(graph.predecessors(pred_var))) == 0:\n",
    "            run_results_df['predicted_variable_node_type'] = ['source_node']\n",
    "            if len(list(graph.successors(pred_var))) == 0:\n",
    "                run_results_df['predicted_variable_node_type'] = ['isolated_node']\n",
    "        else:\n",
    "            if len(list(graph.successors(pred_var))) == 0:\n",
    "                run_results_df['predicted_variable_node_type'] = ['sink_node']\n",
    "            else:\n",
    "                run_results_df['predicted_variable_node_type'] = ['middle_node']\n",
    "        \n",
    "        # XGB CausalDA\n",
    "        if fit_to_aug_only:\n",
    "            if aug_data.shape[0] > 2 : # minimum 10 samples to compute an estimate\n",
    "                predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "                predictor.fit(aug_data, None, aug_weights*len(data), np.array([]))\n",
    "                selected_model_causalDA = predictor.model\n",
    "        else:\n",
    "            predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "            orig_weights = (1 - aug_coeff)*np.ones(len(train_data))\n",
    "            predictor.fit(train_data, aug_data, orig_weights, aug_weights*aug_coeff*len(data))\n",
    "            selected_model_causalDA = predictor.model\n",
    "        if aug_data.shape[0] > 2 : # minimum 10 samples to compute an estimate\n",
    "            run_results_df['MAPE_CausalDA'] = [mean_absolute_percentage_error(test_data[pred_var], selected_model_causalDA.predict(test_data.drop(columns=[pred_var])))]\n",
    "            run_results_df['r2_CausalDA'] = [r2_score(test_data[pred_var], selected_model_causalDA.predict(test_data.drop(columns=[pred_var])))]\n",
    "            run_results_df['n_estimators_CausalDA'] = [selected_model_causalDA.n_estimators]\n",
    "            run_results_df['reg_lambda_CausalDA'] = [selected_model_causalDA.reg_lambda]\n",
    "        else :\n",
    "            run_results_df['MAPE_CausalDA'] = [np.nan]\n",
    "            run_results_df['r2_CausalDA'] = [np.nan]\n",
    "            run_results_df['n_estimators_CausalDA'] = [np.nan]\n",
    "            run_results_df['reg_lambda_CausalDA'] = [np.nan]\n",
    "        \n",
    "        # Baseline\n",
    "        predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "        orig_weights = np.ones(len(train_data))\n",
    "        predictor.fit(train_data, None, orig_weights, np.array([]))\n",
    "        selected_model_baseline = predictor.model\n",
    "        run_results_df['n_estimators_Baseline'] = [selected_model_baseline.n_estimators]\n",
    "        run_results_df['reg_lambda_Baseline'] = [selected_model_baseline.reg_lambda]\n",
    "        run_results_df['MAPE_Baseline'] = [mean_absolute_percentage_error(test_data[pred_var], selected_model_baseline.predict(test_data.drop(columns=[pred_var])))]\n",
    "        run_results_df['r2_Baseline'] = [r2_score(test_data[pred_var], selected_model_baseline.predict(test_data.drop(columns=[pred_var])))]                  \n",
    "    \n",
    "        # Fill results table\n",
    "        aggregated_results_df = pd.concat([aggregated_results_df,pd.DataFrame.from_dict(run_results_df)], ignore_index=True)\n",
    "    return aggregated_results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-1- Influence des paramètres du problème sur la méthode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Default parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose wether to use light parameters \n",
    "use_light_param = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### SCM simulation\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "noise_type = 'gaussian'\n",
    "\n",
    "### Dataset\n",
    "train_size = 0.7\n",
    "\n",
    "### Global Parameters\n",
    "param_grid_search = {\n",
    "    'n_estimators': [10, 50, 200],\n",
    "    'reg_lambda': [1, 10, 100]\n",
    "}\n",
    "cv = 3\n",
    "metric_cv = mean_squared_error\n",
    "save_models = False\n",
    "\n",
    "### CausalDA Parameters\n",
    "augmenter_config_name = 'FullAugment'\n",
    "augmenter_config = {\n",
    "    'normalize_threshold_by_data_size': True,\n",
    "    'weight_threshold': 1e-2, \n",
    "    'weight_threshold_type': 'total',\n",
    "    'weight_kernel_cfg': {\n",
    "        'type': 'vanilla_kernel',\n",
    "        'conti_kertype': 'gaussian',\n",
    "        'conti_bw_method': 'normal_reference',\n",
    "        'conti_bw_temperature': 1,\n",
    "        'ordered_kertype': 'indicator',\n",
    "        'ordered_bw_method': 'indicator',\n",
    "        'unordered_kertype': 'indicator',\n",
    "        'unordered_bw_method': 'indicator',\n",
    "        'const_bandwidth': False,\n",
    "        'bandwidth_temperature': 0.001\n",
    "    }\n",
    "}\n",
    "fit_to_aug_only = True\n",
    "\n",
    "### Evaluation parameters\n",
    "evaluators_param = [(r2_score,'r2'),\n",
    "                    (mean_absolute_percentage_error, 'MAPE')\n",
    "                    ]\n",
    "\n",
    "save_data = False\n",
    "debug = False\n",
    "\n",
    "nb_repetition = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_agg_res = ['run_id', \n",
    "                   'mechanism', \n",
    "                   'nb_nodes',\n",
    "                   'expected_degree',\n",
    "                   'size_longest_path_in_graph',\n",
    "                   'noise_coeff',\n",
    "                   'nb_observations',\n",
    "                   'predicted_variable',\n",
    "                   'predicted_variable_node_type',\n",
    "                   'weight_threshold',\n",
    "                   'frac_filtered',\n",
    "                   'frac_augmented',\n",
    "                   'avg_weight_aug_in_train',\n",
    "                   'avg_weight_aug_NOT_in_train',\n",
    "                   'sum_weight_aug_in_train',\n",
    "                   'sum_weight_aug_NOT_in_train',\n",
    "                   'SUM_aug_weights',\n",
    "                   'MAPE_CausalDA',\n",
    "                   'MAPE_Baseline',\n",
    "                   'r2_CausalDA',\n",
    "                   'r2_Baseline',\n",
    "                   'n_estimators_CausalDA',\n",
    "                   'n_estimators_Baseline',\n",
    "                   'reg_lambda_CausalDA',\n",
    "                   'reg_lambda_Baseline',\n",
    "                   'KL_div_train_vs_aug_NOT_weighted',\n",
    "                   'KL_div_train_vs_aug_weighted',\n",
    "                   'Wasserstein_dist_train_vs_aug_NOT_weighted',\n",
    "                   'Wasserstein_dist_train_vs_aug_weighted',\n",
    "                   'variance_train_NOT_weighted',\n",
    "                   'variance_aug_NOT_weighted',\n",
    "                   'variance_aug_weighted'\n",
    "                    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-a Mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanism_list = ['linear', 'polynomial', 'sigmoid_mix', 'gp_add', 'gp_mix', 'nn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    mechanism_list = ['linear', 'polynomial']\n",
    "    nb_obs = 100\n",
    "    nb_var = 5\n",
    "    expected_degree = 2\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_mechanisms = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "\n",
    "for mech_i in tqdm(range(len(mechanism_list))):\n",
    "    mech = mechanism_list[mech_i]\n",
    "    print(mech)\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on mechanism \" + mech)\n",
    "        run_results_mechanisms = {}\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'mechanisms_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_mechanisms['run_id'] = [i]\n",
    "        run_results_mechanisms['mechanism'] = [mech]\n",
    "        run_results_mechanisms['weight_threshold'] = [augmenter_config['weight_threshold']]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=mech,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_mechanisms['nb_nodes'] = [nb_var]\n",
    "        run_results_mechanisms['noise_coeff'] = [noise_coeff]\n",
    "        run_results_mechanisms['expected_degree'] = expected_degree\n",
    "        run_results_mechanisms['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_mechanisms['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_mechanisms = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_mechanisms,\n",
    "                                                                          aggregated_results_df=aggregated_results_mechanisms)\n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_mechanisms_res_' + mech + '.csv'\n",
    "    aggregated_results_mechanisms.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/mechanisms_res.csv'\n",
    "aggregated_results_mechanisms.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-b Problem dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions_list = [7, 8, 9, 10, 15, 20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    nb_obs = 10\n",
    "    causal_mechanism = 'linear'\n",
    "    expected_degree = 2\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1\n",
    "    dimensions_list = [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_dimensions = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "\n",
    "for dim_i in tqdm(range(len(dimensions_list))):\n",
    "    dim = dimensions_list[dim_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on dimension \" + str(dim))\n",
    "        run_results_dimensions= {}\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'dimension_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_dimensions['run_id'] = [i]\n",
    "        run_results_dimensions['mechanism'] = [causal_mechanism]\n",
    "        run_results_dimensions['weight_threshold'] = [augmenter_config['weight_threshold']]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=dim,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_dimensions['nb_nodes'] = [dim]\n",
    "        run_results_dimensions['noise_coeff'] = [noise_coeff]\n",
    "        run_results_dimensions['expected_degree'] = [expected_degree]\n",
    "        run_results_dimensions['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_dimensions['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_dimensions = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_dimensions,\n",
    "                                                                          aggregated_results_df=aggregated_results_dimensions)\n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_dimensions_res_' + str(dim) + '.csv'\n",
    "    aggregated_results_dimensions.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/dimensions_res.csv'\n",
    "aggregated_results_dimensions.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-c Graph density - expected degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_var = 15\n",
    "density_list = [0, 1, 2, 3, 4, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    nb_obs = 10\n",
    "    nb_var = 10\n",
    "    density_list = [3, 4]\n",
    "    causal_mechanism = 'linear'\n",
    "    expected_degree = 2\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_graph_density = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "\n",
    "for density_i in tqdm(range(len(density_list))):\n",
    "    density = density_list[density_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on density \" + str(density))\n",
    "        run_results_graph_density= {}\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'graph_density_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_graph_density['run_id'] = [i]\n",
    "        run_results_graph_density['mechanism'] = [causal_mechanism]\n",
    "        run_results_graph_density['weight_threshold'] = [augmenter_config['weight_threshold']]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=density,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_graph_density['nb_nodes'] = [nb_var]\n",
    "        run_results_graph_density['noise_coeff'] = [noise_coeff]\n",
    "        run_results_graph_density['expected_degree'] = [density]\n",
    "        run_results_graph_density['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_graph_density['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_graph_density = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_graph_density,\n",
    "                                                                          aggregated_results_df=aggregated_results_graph_density)    \n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_graph_density_res_' + str(density) + '.csv'\n",
    "    aggregated_results_graph_density.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save full table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/graph_density_res.csv'\n",
    "aggregated_results_graph_density.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-d SCM noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_coeff_list = [0.1, 0.2, 0.4, 0.6, 0.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    nb_obs = 10\n",
    "    nb_var = 5\n",
    "    causal_mechanism = 'linear'\n",
    "    expected_degree = 2\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1\n",
    "    noise_coeff_list = [0.1, 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_SCM_noise = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "for noise_coeff_i in tqdm(range(len(noise_coeff_list))):\n",
    "    noise_coeff = noise_coeff_list[noise_coeff_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on noise_coeff \" + str(noise_coeff))\n",
    "        run_results_SCM_noise= {}\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'noise_coeff_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_SCM_noise['run_id'] = [i]\n",
    "        run_results_SCM_noise['mechanism'] = [causal_mechanism]\n",
    "        run_results_SCM_noise['weight_threshold'] = [augmenter_config['weight_threshold']]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_SCM_noise['nb_nodes'] = [nb_var]\n",
    "        run_results_SCM_noise['noise_coeff'] = [noise_coeff]\n",
    "        run_results_SCM_noise['expected_degree'] = [expected_degree]\n",
    "        run_results_SCM_noise['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_SCM_noise['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_SCM_noise = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_SCM_noise,\n",
    "                                                                          aggregated_results_df=aggregated_results_SCM_noise)    \n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_noise_coeff_res_' + str(noise_coeff) + '.csv'\n",
    "    aggregated_results_SCM_noise.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save full table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/noise_coeff_res.csv'\n",
    "aggregated_results_SCM_noise.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-e Dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_obs_list = [30] + [i for i in range(40, 100, 20)] + [i for i in range(100, 800, 200)]\n",
    "print(nb_obs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    nb_obs_list = [10, 15]\n",
    "    nb_var = 5\n",
    "    causal_mechanism = 'linear'\n",
    "    expected_degree = 2\n",
    "    noise_coeff = 0.4\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_nb_obs = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "for nb_obs_i in tqdm(range(len(nb_obs_list))):\n",
    "    nb_obs = nb_obs_list[nb_obs_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on nb_obs \" + str(nb_obs))\n",
    "        run_results_nb_obs= {}\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'nb_observations_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_nb_obs['run_id'] = [i]\n",
    "        run_results_nb_obs['mechanism'] = [causal_mechanism]\n",
    "        run_results_nb_obs['weight_threshold'] = [augmenter_config['weight_threshold']]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_nb_obs['nb_nodes'] = [nb_var]\n",
    "        run_results_nb_obs['noise_coeff'] = [noise_coeff]\n",
    "        run_results_nb_obs['expected_degree'] = [expected_degree]\n",
    "        run_results_nb_obs['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_nb_obs['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_nb_obs = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_nb_obs,\n",
    "                                                                          aggregated_results_df=aggregated_results_nb_obs)    \n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_nb_observations_res_' + str(nb_obs) + '.csv'\n",
    "    aggregated_results_nb_obs.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save full table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/nb_observations_res.csv'\n",
    "aggregated_results_nb_obs.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II-1-f Weight threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [1/(10**i) for i in range(1, 6)]\n",
    "print(threshold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    threshold_list = [1/(10**i) for i in range(2, 4)]\n",
    "    nb_var = 5\n",
    "    causal_mechanism = 'linear'\n",
    "    expected_degree = 2\n",
    "    noise_coeff = 0.4\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_threshold = pd.DataFrame(columns = columns_agg_res)\n",
    "\n",
    "for threshold_i in tqdm(range(len(threshold_list))):\n",
    "    threshold = threshold_list[threshold_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print(\"!!NEW!! :Repetition \" + str(i) + \" on threshold \" + str(threshold))\n",
    "        run_results_threshold= {}\n",
    "        \n",
    "        augmenter_config['weight_threshold'] = threshold\n",
    "        run_results_threshold['weight_threshold'] = [threshold]\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'threshold_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = mech + '_run_' + str(i)\n",
    "        run_results_threshold['run_id'] = [i]\n",
    "        run_results_threshold['mechanism'] = [causal_mechanism]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        run_results_threshold['nb_nodes'] = [nb_var]\n",
    "        run_results_threshold['noise_coeff'] = [noise_coeff]\n",
    "        run_results_threshold['expected_degree'] = [density]\n",
    "        run_results_threshold['nb_observations'] = [nb_obs]\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        run_results_threshold['size_longest_path_in_graph'] = [len(nx.dag_longest_path(graph))]\n",
    "        graph_ = (data.columns,list(graph.edges),[])\n",
    "        \n",
    "        ### Augment data, fit and evaluate XGB\n",
    "        aggregated_results_threshold = augment_data_fit_and_evaluate_xgb(data=data,\n",
    "                                                                          train_size=train_size,\n",
    "                                                                          augmenter_config_name=augmenter_config_name,\n",
    "                                                                          augmenter_config=augmenter_config,\n",
    "                                                                          graph_=graph_,\n",
    "                                                                          data_cache_base_path=data_cache_base_path,\n",
    "                                                                          data_cache_name=data_cache_name,\n",
    "                                                                          save_data=save_data,\n",
    "                                                                          run_results_df=run_results_threshold,\n",
    "                                                                          aggregated_results_df=aggregated_results_threshold)    \n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_threshold_res_' + str(threshold) + '.csv'\n",
    "    aggregated_results_threshold.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save full table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/threshold_res.csv'\n",
    "aggregated_results_threshold.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_agg_res_outliers = ['run_id', \n",
    "                                'predicted_variable',\n",
    "                                'predicted_variable_node_type',\n",
    "                                \n",
    "                                'frac_outliers',\n",
    "                                'frac_outliers_filtered',\n",
    "                                \n",
    "                                'frac_filtered_true',\n",
    "                                'frac_augmented_true',\n",
    "                                'avg_weight_aug_in_train_true',\n",
    "                                'avg_weight_aug_NOT_in_train_true',\n",
    "                                'SUM_aug_weights_true',\n",
    "                                'frac_filtered_wrong',\n",
    "                                'frac_augmented_wrong',\n",
    "                                'avg_weight_aug_in_train_wrong',\n",
    "                                'avg_weight_aug_NOT_in_train_wrong',\n",
    "                                'SUM_aug_weights_wrong',\n",
    "                                \n",
    "                                'MAPE_CausalDA_true',\n",
    "                                'MAPE_CausalDA_wrong',\n",
    "                                'MAPE_Baseline',\n",
    "                                'r2_CausalDA_true',\n",
    "                                'r2_CausalDA_wrong',\n",
    "                                'r2_Baseline',\n",
    "                                'n_estimators_CausalDA_true',\n",
    "                                'n_estimators_CausalDA_wrong',\n",
    "                                'n_estimators_Baseline',\n",
    "                                'reg_lambda_CausalDA_true',\n",
    "                                'reg_lambda_CausalDA_wrong',\n",
    "                                'reg_lambda_Baseline',\n",
    "                                \n",
    "                                'KL_div_train_vs_aug_weighted_true',\n",
    "                                'KL_div_train_vs_aug_weighted_wrong',\n",
    "                                'KL_div_aug_true_vs_aug_weighted_wrong',\n",
    "                                'Wasserstein_dist_train_vs_aug_weighted_true',\n",
    "                                'Wasserstein_dist_train_vs_aug_weighted_wrong',\n",
    "                                'Wasserstein_dist_aug_true_vs_aug_weighted_wrong',\n",
    "                                'variance_train_NOT_weighted',\n",
    "                                'variance_aug_weighted_true',\n",
    "                                'variance_aug_weighted_wrong'                 \n",
    "                                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_outliers_list = [i/100 for i in range(1, 5)] + [i/100 for i in range(5, 20, 5)]\n",
    "print(frac_outliers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light parameters for testing\n",
    "\n",
    "if use_light_param:\n",
    "    nb_var = 5\n",
    "    frac_outliers_list = [0.2, 0.3]\n",
    "    causal_mechanism = 'linear'\n",
    "    noise_type = 'gaussian'\n",
    "    nb_obs=10\n",
    "    expected_degree = 2\n",
    "    noise_coeff = 0.4\n",
    "    param_grid_search = {\n",
    "        'n_estimators': [5, 10],\n",
    "        'reg_lambda': [1, 10]\n",
    "    }\n",
    "    cv = 2\n",
    "    save_data = False\n",
    "    nb_repetition = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_outliers = pd.DataFrame(columns = columns_agg_res_outliers)\n",
    "\n",
    "for frac_outliers_i in tqdm(range(len(frac_outliers_list))):\n",
    "    frac_outliers = frac_outliers_list[frac_outliers_i]\n",
    "    for i in range(nb_repetition):\n",
    "        print('Repetition ' + str(i) + ' frac_outliers ' + str(frac_outliers))\n",
    "        run_results_frac_outliers= {}\n",
    "        run_results_frac_outliers['frac_outliers'] = [frac_outliers]\n",
    "        \n",
    "        ### run_id and name\n",
    "        data_cache_base_path = 'outliers_variation'\n",
    "        data_cache_base_path = Path(data_cache_base_path)\n",
    "        data_cache_name = str(frac_outliers) + '_run_' + str(i)\n",
    "        run_results_frac_outliers['run_id'] = [i]\n",
    "        \n",
    "        ### SCM creation & Data generation\n",
    "        data = causal_data_simu.generate_data(\n",
    "                causal_mechanism=causal_mechanism,\n",
    "                nodes=nb_var,\n",
    "                noise=noise_type,\n",
    "                noise_coeff=noise_coeff,\n",
    "                npoints=nb_obs,\n",
    "                expected_degree=expected_degree,\n",
    "                dag_type='erdos'\n",
    "            )\n",
    "        graph = causal_data_simu.load_graph(\"graph.gpickle\")\n",
    "        graph_ = (data.columns,graph.edges,[])\n",
    "        \n",
    "        ### Build the dataset\n",
    "        predicted_var_name = data.columns[-1] # Fake varaible to predict --> only to use train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.drop(columns=[predicted_var_name]), data[predicted_var_name], train_size=train_size)\n",
    "        train_data = pd.concat((X_train, y_train), axis=1)\n",
    "        test_data = pd.concat((X_test, y_test), axis=1)\n",
    "        \n",
    "        # Randomly add uniformly distributed outliers\n",
    "        train_data_wrong, outliers_index_list = causal_data_simu.add_uniform_outliers(train_data, frac_outlier=frac_outliers)\n",
    "        outliers_df = train_data_wrong.iloc[outliers_index_list]\n",
    "        \n",
    "        ### Augment data TRUE\n",
    "        AugmenterConfigClass = getattr(method_config_module, augmenter_config_name)\n",
    "        augmenter_config_ = AugmenterConfigClass(**augmenter_config)\n",
    "        aug_data_true, aug_weights_true = _augment(train_data, \n",
    "                                                    graph_, \n",
    "                                                    augmenter_config_, \n",
    "                                                    data_cache_base_path, \n",
    "                                                    data_cache_name)\n",
    "        \n",
    "        ### Augment data WRONG\n",
    "        aug_data_wrong, aug_weights_wrong = _augment(train_data_wrong, \n",
    "                                                    graph_, \n",
    "                                                    augmenter_config_, \n",
    "                                                    data_cache_base_path, \n",
    "                                                    data_cache_name)\n",
    "\n",
    "        ### Compute stats on augmented TRUE data VS train data : \n",
    "        aug_data_df_true = aug_data_true.copy()\n",
    "        aug_data_df_true['aug_weights'] = aug_weights_true\n",
    "        train_df_join_aug_true = train_data.merge(aug_data_df_true, how=\"left\", on=list(train_data.columns))\n",
    "        run_results_frac_outliers['frac_filtered_true'] = [train_df_join_aug_true[train_df_join_aug_true['aug_weights'].isna()].shape[0] / train_data.shape[0]]\n",
    "        \n",
    "        train_data_df = train_data.copy()\n",
    "        train_data_df['col'] = 1\n",
    "        aug_df_true_join_train = aug_data_df_true.merge(train_data_df, how=\"left\", on=list(aug_data_true.columns))\n",
    "        run_results_frac_outliers['frac_augmented_true'] = [aug_df_true_join_train[aug_df_true_join_train['col'].isna()].shape[0] / train_data.shape[0]]\n",
    "        \n",
    "        run_results_frac_outliers['avg_weight_aug_in_train_true'] = [aug_df_true_join_train[aug_df_true_join_train['col']==1].aug_weights.mean()]\n",
    "        run_results_frac_outliers['avg_weight_aug_NOT_in_train_true'] = [aug_df_true_join_train[aug_df_true_join_train['col'].isna()].aug_weights.mean()]\n",
    "        run_results_frac_outliers['SUM_aug_weights_true'] = [aug_data_df_true['aug_weights'].sum()]\n",
    "        \n",
    "        run_results_frac_outliers['variance_train_NOT_weighted'] = [np.var(train_data.to_numpy(), axis=0)]\n",
    "        run_results_frac_outliers['variance_aug_weighted_true'] = [DescrStatsW(aug_data_true.to_numpy(), weights=aug_weights_true, ddof=0).var]\n",
    "        \n",
    "        if aug_data_true.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "            run_results_frac_outliers['KL_div_train_vs_aug_weighted_true'] = [KL_div(aug_data_true.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights_true)]\n",
    "            run_results_frac_outliers['Wasserstein_dist_train_vs_aug_weighted_true'] = [wasserstein_distance(aug_data_true.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights_true)]\n",
    "        \n",
    "        ### Compute stats on augmented WRONG data VS train data : \n",
    "        aug_data_df_wrong = aug_data_wrong.copy()\n",
    "        aug_data_df_wrong['aug_weights'] = aug_weights_wrong\n",
    "        train_df_join_aug_wrong = train_data.merge(aug_data_df_wrong, how=\"left\", on=list(train_data.columns))\n",
    "        run_results_frac_outliers['frac_filtered_wrong'] = [train_df_join_aug_wrong[train_df_join_aug_wrong['aug_weights'].isna()].shape[0] / train_data.shape[0]]\n",
    "        \n",
    "        outliers_df_join_aug_wrong = outliers_df.merge(aug_data_df_wrong, how=\"left\", on=list(outliers_df.columns))\n",
    "        run_results_frac_outliers['frac_outliers_filtered'] = [outliers_df_join_aug_wrong[outliers_df_join_aug_wrong['aug_weights'].isna()].shape[0] / outliers_df.shape[0]]\n",
    "        \n",
    "        train_data_df = train_data.copy()\n",
    "        train_data_df['col'] = 1\n",
    "        aug_df_wrong_join_train = aug_data_df_wrong.merge(train_data_df, how=\"left\", on=list(aug_data_wrong.columns))\n",
    "        run_results_frac_outliers['frac_augmented_wrong'] = [aug_df_wrong_join_train[aug_df_wrong_join_train['col'].isna()].shape[0] / train_data.shape[0]]\n",
    "        \n",
    "        run_results_frac_outliers['avg_weight_aug_in_train_wrong'] = [aug_df_wrong_join_train[aug_df_wrong_join_train['col']==1].aug_weights.mean()]\n",
    "        run_results_frac_outliers['avg_weight_aug_NOT_in_train_wrong'] = [aug_df_wrong_join_train[aug_df_wrong_join_train['col'].isna()].aug_weights.mean()]\n",
    "        run_results_frac_outliers['SUM_aug_weights_wrong'] = [aug_data_df_wrong['aug_weights'].sum()]\n",
    "\n",
    "        run_results_frac_outliers['variance_aug_weighted_wrong'] = [DescrStatsW(aug_data_wrong.to_numpy(), weights=aug_weights_wrong, ddof=0).var]\n",
    "        \n",
    "        if aug_data_wrong.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "            run_results_frac_outliers['KL_div_train_vs_aug_weighted_wrong'] = [KL_div(aug_data_wrong.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights_wrong)]\n",
    "            run_results_frac_outliers['Wasserstein_dist_train_vs_aug_weighted_wrong'] = [wasserstein_distance(aug_data_wrong.to_numpy(), train_data.to_numpy(), kernel='gaussian', x_weights=aug_weights_wrong)]\n",
    "\n",
    "        ### Compute stats on augmented TRUE VS WRONG\n",
    "        if (aug_data_true.shape[0] > 2) & (aug_data_wrong.shape[0] > 2) : # minimum 2 samples to compute an estimate\n",
    "            run_results_frac_outliers['KL_div_aug_true_vs_aug_weighted_wrong'] = [KL_div(aug_data_wrong.to_numpy(), \n",
    "                                                                                              aug_data_true.to_numpy(), \n",
    "                                                                                              kernel='gaussian', \n",
    "                                                                                              x_weights=aug_weights_wrong, \n",
    "                                                                                              y_weights=aug_weights_true)]\n",
    "            run_results_frac_outliers['Wasserstein_dist_aug_true_vs_aug_weighted_wrong'] = [wasserstein_distance(aug_data_wrong.to_numpy(), \n",
    "                                                                                                                      aug_data_true.to_numpy(), \n",
    "                                                                                                                      kernel='gaussian', \n",
    "                                                                                                                      x_weights=aug_weights_wrong, \n",
    "                                                                                                                      y_weights=aug_weights_true)]\n",
    "        \n",
    "        ### Fit XGB models\n",
    "        for pred_var in train_data.columns:\n",
    "        \n",
    "            # predicted variable caracteristics\n",
    "            run_results_frac_outliers['predicted_variable'] = [pred_var]\n",
    "            if len(list(graph.predecessors(pred_var))) == 0:\n",
    "                run_results_frac_outliers['predicted_variable_node_type'] = ['source_node']\n",
    "                if len(list(graph.successors(pred_var))) == 0:\n",
    "                    run_results_frac_outliers['predicted_variable_node_type'] = ['isolated_node']\n",
    "            else:\n",
    "                if len(list(graph.successors(pred_var))) == 0:\n",
    "                    run_results_frac_outliers['predicted_variable_node_type'] = ['sink_node']\n",
    "                else:\n",
    "                    run_results_frac_outliers['predicted_variable_node_type'] = ['middle_node']\n",
    "            \n",
    "            # XGB CausalDA TRUE\n",
    "            if fit_to_aug_only:\n",
    "                if aug_data_true.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "                    predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "                    predictor.fit(aug_data_true, None, aug_weights_true*len(train_data), np.array([]))\n",
    "                    selected_model_causalDA_true = predictor.model\n",
    "            else:\n",
    "                predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "                orig_weights = (1 - aug_coeff)*np.ones(len(train_data))\n",
    "                predictor.fit(train_data, aug_data_true, orig_weights, aug_weights_true*aug_coeff*len(train_data))\n",
    "                selected_model_causalDA_true = predictor.model\n",
    "            if aug_data_true.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "                run_results_frac_outliers['MAPE_CausalDA_true'] = [mean_absolute_percentage_error(test_data[pred_var], selected_model_causalDA_true.predict(test_data.drop(columns=[pred_var])))]\n",
    "                run_results_frac_outliers['r2_CausalDA_true'] = [r2_score(test_data[pred_var], selected_model_causalDA_true.predict(test_data.drop(columns=[pred_var])))]\n",
    "                run_results_frac_outliers['n_estimators_CausalDA_true'] = [selected_model_causalDA_true.n_estimators]\n",
    "                run_results_frac_outliers['reg_lambda_CausalDA_true'] = [selected_model_causalDA_true.reg_lambda]\n",
    "            else :\n",
    "                run_results_frac_outliers['MAPE_CausalDA_true'] = [np.nan]\n",
    "                run_results_frac_outliers['r2_CausalDA_true'] = [np.nan]\n",
    "                run_results_frac_outliers['n_estimators_CausalDA_true'] = [np.nan]\n",
    "                run_results_frac_outliers['reg_lambda_CausalDA_true'] = [np.nan]\n",
    "            \n",
    "            # XGB CausalDA WRONG\n",
    "            if fit_to_aug_only:\n",
    "                if aug_data_wrong.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "                    predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "                    predictor.fit(aug_data_wrong, None, aug_weights_wrong*len(train_data), np.array([]))\n",
    "                    selected_model_causalDA_wrong = predictor.model\n",
    "            else:\n",
    "                predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "                orig_weights = (1 - aug_coeff)*np.ones(len(train_data))\n",
    "                predictor.fit(train_data, aug_data_wrong, orig_weights, aug_weights_wrong*aug_coeff*len(train_data))\n",
    "                selected_model_causalDA_wrong = predictor.model\n",
    "            if aug_data_wrong.shape[0] > 2 : # minimum 2 samples to compute an estimate\n",
    "                run_results_frac_outliers['MAPE_CausalDA_wrong'] = [mean_absolute_percentage_error(test_data[pred_var], selected_model_causalDA_wrong.predict(test_data.drop(columns=[pred_var])))]\n",
    "                run_results_frac_outliers['r2_CausalDA_wrong'] = [r2_score(test_data[pred_var], selected_model_causalDA_wrong.predict(test_data.drop(columns=[pred_var])))]\n",
    "                run_results_frac_outliers['n_estimators_CausalDA_wrong'] = [selected_model_causalDA_wrong.n_estimators]\n",
    "                run_results_frac_outliers['reg_lambda_CausalDA_wrong'] = [selected_model_causalDA_wrong.reg_lambda]\n",
    "            else :\n",
    "                run_results_frac_outliers['MAPE_CausalDA_wrong'] = [np.nan]\n",
    "                run_results_frac_outliers['r2_CausalDA_wrong'] = [np.nan]\n",
    "                run_results_frac_outliers['n_estimators_CausalDA_wrong'] = [np.nan]\n",
    "                run_results_frac_outliers['reg_lambda_CausalDA_wrong'] = [np.nan]\n",
    "            \n",
    "            # Baseline\n",
    "            predictor = AugXGBRegressor(pred_var, param_grid_search)\n",
    "            orig_weights = np.ones(len(train_data))\n",
    "            predictor.fit(train_data, None, orig_weights, np.array([]))\n",
    "            selected_model_baseline = predictor.model\n",
    "            run_results_frac_outliers['n_estimators_Baseline'] = [selected_model_baseline.n_estimators]\n",
    "            run_results_frac_outliers['reg_lambda_Baseline'] = [selected_model_baseline.reg_lambda]\n",
    "            run_results_frac_outliers['MAPE_Baseline'] = [mean_absolute_percentage_error(test_data[pred_var], selected_model_baseline.predict(test_data.drop(columns=[pred_var])))]\n",
    "            run_results_frac_outliers['r2_Baseline'] = [r2_score(test_data[pred_var], selected_model_baseline.predict(test_data.drop(columns=[pred_var])))]                  \n",
    "        \n",
    "            # Fill results table\n",
    "            aggregated_results_outliers = pd.concat([aggregated_results_outliers,pd.DataFrame.from_dict(run_results_frac_outliers)], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    # save intermediate results\n",
    "    path_to_save_csv = str(data_cache_base_path) + '/intermediate_outliers_res_' + str(frac_outliers) + '.csv'\n",
    "    aggregated_results_outliers.to_csv(path_to_save_csv, index=False)\n",
    "\n",
    "# save full table\n",
    "path_to_save_csv = str(data_cache_base_path) + '/outliers_res.csv'\n",
    "aggregated_results_outliers.to_csv(path_to_save_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset default parameters\n",
    "\n",
    "nb_obs = 500\n",
    "causal_mechanism = 'nn'\n",
    "nb_var = 10\n",
    "expected_degree = 3\n",
    "noise_coeff = 0.4\n",
    "augmenter_config['weight_threshold'] = 1e-2\n",
    "nb_repetition = 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ADMGresults')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cc1f8893ecc801c8eb79af4fcbef525648b7ac781db12c5604c5b6c4d5eafe4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
